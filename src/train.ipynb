{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import importlib\n",
    "from multi_head_attention import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_modules():\n",
    "    importlib.reload(Transformer)\n",
    "    importlib.reload(MultiHeadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TOKEN = '<s>'\n",
    "END_TOKEN = '<\\s>'\n",
    "PADDING_TOKEN = '<pad>'\n",
    "english_vocabulary = [START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "                    '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "                    ':', '<', '=', '>', '?', '@', ';',\n",
    "                    '[', '\\\\', ']',\n",
    "                    '^', '_', '`', \n",
    "                    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
    "                    'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \n",
    "                    'y', 'z', \n",
    "                    '{', '|', '}', '~', PADDING_TOKEN, END_TOKEN\n",
    "                    ]\n",
    "\n",
    "persian_vocabulary = [\n",
    "START_TOKEN, ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', \n",
    "'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';',\n",
    "':', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', \n",
    "'آ', 'ا', 'ب', 'پ', 'ت', 'ث', 'ج', 'چ', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'ژ', 'س', 'ش', \n",
    "'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ک', 'گ', 'ل', 'م', 'ن', 'و', 'ه', 'ی',\n",
    "'ء', 'ۀ', 'ؤ', 'ي', 'ك', 'ة', '‌', 'ٔ', 'ى', PADDING_TOKEN, END_TOKEN\n",
    "]\n",
    "index_to_persian = {k:v for k,v in enumerate(persian_vocabulary)}\n",
    "persian_to_index = {v:k for k,v in enumerate(persian_vocabulary)}\n",
    "index_to_english = {k:v for k,v in enumerate(english_vocabulary)}\n",
    "english_to_index = {v:k for k,v in enumerate(english_vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>persian</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>گلدان روی میز چای حاضر و آماده بود.</td>\n",
       "      <td>the vase filled with water was ready in the ce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آن وقت قاضی چه کرد؟</td>\n",
       "      <td>What did the justice do?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>به روزگار فيلماي ؛ نقطه تلاقي ؛ ماري کثيف يا ه...</td>\n",
       "      <td>vanishing point days , the dirty mary crazy la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>افراد مورد اعتماد زیردستهایشان به عنوان سرپرست...</td>\n",
       "      <td>with the trust of his subordinates as the head...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>زودتر برویم. من حاضرم.</td>\n",
       "      <td>I am ready, my son, said Mercedes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             persian  \\\n",
       "0                گلدان روی میز چای حاضر و آماده بود.   \n",
       "1                                آن وقت قاضی چه کرد؟   \n",
       "2  به روزگار فيلماي ؛ نقطه تلاقي ؛ ماري کثيف يا ه...   \n",
       "3  افراد مورد اعتماد زیردستهایشان به عنوان سرپرست...   \n",
       "4                             زودتر برویم. من حاضرم.   \n",
       "\n",
       "                                             english  \n",
       "0  the vase filled with water was ready in the ce...  \n",
       "1                           What did the justice do?  \n",
       "2  vanishing point days , the dirty mary crazy la...  \n",
       "3  with the trust of his subordinates as the head...  \n",
       "4                 I am ready, my son, said Mercedes.  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../dataset/shortened_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['english'] = df['english'].astype(str)\n",
    "df['persian'] = df['persian'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper_english(x:str):\n",
    "        for c in x:\n",
    "            if not c in english_vocabulary:\n",
    "                x = x.replace(c, '')\n",
    "        return x\n",
    "\n",
    "def helper_persian(x:str):\n",
    "    for c in x:\n",
    "        if not c in persian_vocabulary:\n",
    "            x = x.replace(c, '')\n",
    "    return x\n",
    "\n",
    "df['english'] = df['english'].apply(str.lower)\n",
    "df['english'] = df['english'].apply(helper_english)\n",
    "df['persian'] = df['persian'].apply(helper_persian)\n",
    "persian_sentences = df['persian'].to_list()\n",
    "english_sentences = df['english'].to_list()\n",
    "enlish_sentences = df['english'].to_list()\n",
    "persian_sentences = df['persian'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim = 512\n",
    "batch_size = 30\n",
    "hidden_fc = 2048\n",
    "num_heads = 8\n",
    "drop_prob = 0.1\n",
    "num_layers = 1\n",
    "max_sequence_length = 200\n",
    "persian_vocab_size = len(persian_vocabulary)\n",
    "\n",
    "transformer = Transformer((batch_size, max_sequence_length, model_dim),\n",
    "                          model_dim, \n",
    "                          hidden_fc,\n",
    "                          num_heads, \n",
    "                          drop_prob, \n",
    "                          num_layers, \n",
    "                          max_sequence_length,\n",
    "                          persian_vocab_size,\n",
    "                          english_to_index,\n",
    "                          persian_to_index,\n",
    "                          START_TOKEN, \n",
    "                          END_TOKEN, \n",
    "                          PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslateDataset(Dataset):\n",
    "    def __init__(self, english_sentences, persian_sentences):\n",
    "        super().__init__()\n",
    "        self.english_sentences = english_sentences\n",
    "        self.persian_sentences = persian_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.persian_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.english_sentences[idx], self.persian_sentences[idx]    \n",
    "    \n",
    "\n",
    "dataset = TranslateDataset(english_sentences, persian_sentences)\n",
    "train_loader = DataLoader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=persian_to_index[PADDING_TOKEN])\n",
    "for params in transformer.parameters():\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=1e-4)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NEG_INFTY = -1e9  \n",
    "\n",
    "def create_masks(eng_batch, persian_batch, number_of_heads):  \n",
    "    num_sentences = len(eng_batch)  \n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length], True)  \n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)  \n",
    "    \n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)  \n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)  \n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length], False)  \n",
    "\n",
    "    for idx in range(num_sentences):  \n",
    "        eng_sentence_length, persian_sentence_length = len(eng_batch[idx]), len(persian_batch[idx])  \n",
    "        eng_chars_to_padding_mask = np.arange(eng_sentence_length, max_sequence_length)  \n",
    "        persian_chars_to_padding_mask = np.arange(persian_sentence_length, max_sequence_length)  \n",
    "\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True  \n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True  \n",
    "        \n",
    "        decoder_padding_mask_self_attention[idx, persian_chars_to_padding_mask, :] = True  \n",
    "        decoder_padding_mask_self_attention[idx, :, persian_chars_to_padding_mask] = True  \n",
    "        \n",
    "        decoder_padding_mask_cross_attention[idx, persian_chars_to_padding_mask, :] = True  \n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True  \n",
    "\n",
    "    # Create self-attention masks  \n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)  \n",
    "    decoder_self_attention_mask = torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)  \n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)  \n",
    "\n",
    "    # Expand the masks to have the number of heads  \n",
    "    encoder_self_attention_mask = encoder_self_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)  \n",
    "    decoder_self_attention_mask = decoder_self_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)  \n",
    "    decoder_cross_attention_mask = decoder_cross_attention_mask.unsqueeze(1).repeat(1, number_of_heads, 1, 1)  \n",
    "\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_batch = ['hello', 'how are you']\n",
    "persian_batch = ['سلام', 'چطوری']\n",
    "encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, persian_batch, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 200, 200])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_self_attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Iteration 0 : 0.003514297306537628\n",
      "English: the vase filled with water was ready in the center of the tea table.\n",
      "Persian Translation: گلدان روی میز چای حاضر و آماده بود.\n",
      "Persian Prediction: >هظ*ك/2$1غ\"تظ]ص<_#ۀ#&ا\n",
      "Evaluation translation (should we go to the mall?) : ('ۀ/ظ*?/2$1غ\"ن5لص%_#خ9&ا<\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 100 : 0.0025248141027987003\n",
      "English: including you .\n",
      "Persian Translation: تو را هم شامل ميشه .\n",
      "Persian Prediction: اا                                             ا                                            ا       ی  ایا  ا\n",
      "Evaluation translation (should we go to the mall?) : ('اا                                             ا                                            ا       یر  یا  ن<\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 200 : 0.002507735276594758\n",
      "English: miguel centellas of pronto* takes a look at the worrisome inflation rate in bolivia, which has affected the lower middle classes and small business owners the most.\n",
      "Persian Translation: میگل سنتلاس می‌نویسد که بسیاری از بولیوی‌ها نگران تورم در کوشر هستند که به گفته بسیاری از منابع حدود یازده در صد است.\n",
      "Persian Prediction: اا                                                                                                                  ا                  ی        ر     د ا م اا ش    \n",
      "Evaluation translation (should we go to the mall?) : ('اا                                                                                                       ا          ا             گ    ی   ا    ر  ش  د ا م ای ر    <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 300 : 0.0020809387788176537\n",
      "English: while the other side was the land as they had always known it.\n",
      "Persian Translation: طرف دیگرش همان زمینی بود که می‌شناختند.\n",
      "Persian Prediction: اا                                                                                                                                                          اا ر    \n",
      "Evaluation translation (should we go to the mall?) : ('اا                                                                                                                                                          اا ر    <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 400 : 0.0020199529826641083\n",
      "English: you can get all the t shirts, socks, and underpants at the same stockist\n",
      "Persian Translation: می‌توانی تمام ژاکت‌ها جوراب‌ها و لباس‌های زیر را ازیکجا خریداری کنی\n",
      "Persian Prediction: او                                                                                                                                                          ا  ر                ا ت ا ا     انانااشر ن\n",
      "Evaluation translation (should we go to the mall?) : ('او                                                                                                                                                          ا  ر                ا   ا ا    ن<\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 500 : 0.0025584311224520206\n",
      "English: they know we won't follow them.\n",
      "Persian Translation: مي دونن كه ما ازشون اطاعت نمي كنيم\n",
      "Persian Prediction: اا                                                                                                                                                                              ا   ا ا      نا اا    \n",
      "Evaluation translation (should we go to the mall?) : ('اا                                                                                                                                                          ا                   ا   ا ا      نا اا    <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 600 : 0.002264696639031172\n",
      "English: 'favell was right about wanting a drink,' he said.\n",
      "Persian Translation: در عین حال روی به من کرده و گفت: من هم مثل فیول هوس گیلاسی مشروب کرده‌ام.\n",
      "Persian Prediction: او                                                                                                                                                                              ا     ا       ا اا    \n",
      "Evaluation translation (should we go to the mall?) : ('اا                                                                                                                                                                              ا     ا      نا اا    <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 700 : 0.001720560365356505\n",
      "English: her heart squeezed to a wedge of pain because the poor little fellow fought so for every breath.\n",
      "Persian Translation: از دیدن اینکه موجود بینوای کوچولو برای هر نفسی تقلا می‌کند قلبش فشرده می‌شد.\n",
      "Persian Prediction: اا                                                                                                                                                                              ا     ا       ا       \n",
      "Evaluation translation (should we go to the mall?) : ('ار         <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 800 : 0.0025199460797011852\n",
      "English: it is quite plain, and it never shows a scratch or sign of wear.'\n",
      "Persian Translation: کاملا صاف است و هیچ وقت خراش بر نمی‌دارد یا سایده نمی‌شود.\n",
      "Persian Prediction: اا                                                                                                                                                                              ا             ا       \n",
      "Evaluation translation (should we go to the mall?) : ('ار        <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 900 : 0.0019783403258770704\n",
      "English: run, waiter!\n",
      "Persian Translation: گفت به ملازم که تند رفته ببیند چرا این‌طور زنگ زدند.\n",
      "Persian Prediction: او                                                   \n",
      "Evaluation translation (should we go to the mall?) : ('ار      <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1000 : 0.0017936712829396129\n",
      "English: prepare a horse!\n",
      "Persian Translation: يه اسب براش بياريد\n",
      "Persian Prediction: ما                \n",
      "Evaluation translation (should we go to the mall?) : ('ار   <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1100 : 0.0021455930545926094\n",
      "English: i am not a hostile.\n",
      "Persian Translation: .من دشمن نيستم\n",
      "Persian Prediction: اا            \n",
      "Evaluation translation (should we go to the mall?) : ('ار   <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1200 : 0.001772718969732523\n",
      "English: he felt as if he could walk for miles.\n",
      "Persian Translation: احساس می‌کرد می‌تواند کیلومترها قدم بزند.\n",
      "Persian Prediction: اا                                       \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1300 : 0.002334154909476638\n",
      "English: messieurs, i have news for you.\n",
      "Persian Translation: مسی و خب رهای برایتان دارم\n",
      "Persian Prediction: اا                        \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1400 : 0.0017886729910969734\n",
      "English: have you not provided me with an image of yourself and set it in the sky?\n",
      "Persian Translation: این تو نیستی که تصویری از خود برایم در آسمان نقش می‌زنی\n",
      "Persian Prediction: اا                                                     \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1500 : 0.0016566021367907524\n",
      "English: it had been long, and to stu, with his east texas background, it had seemed fantastically hard.\n",
      "Persian Translation: زمستانی طولانی سپری شده بود وبرای است و که ازمنطقه گرمسیرشرق تگزاس آمده بودزمستان بیش ازحد طول کشیده بود.\n",
      "Persian Prediction: اا                                                                                                         \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1600 : 0.0021412267815321684\n",
      "English: so you ain't had no meat nor bread to eat all this time?\n",
      "Persian Translation: پس تو تا این وقت نه نون خورده بودی نه گوشت!\n",
      "Persian Prediction: اا                         ا               \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1700 : 0.0018555084243416786\n",
      "English: without thinking, i turn my right hand into a blade, drop to one knee and slash at the demon's underbelly.\n",
      "Persian Translation: بدون این که فکر کنم دست راستم را به یک تیغه تیز تبدیل می‌کنم روی یک زانو پایین می‌روم و تیغه را زیر شکم هیولا می‌برم.\n",
      "Persian Prediction: اا                         ا                                                                                          \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n",
      "Iteration 1800 : 0.002012088894844055\n",
      "English: proficiency .\n",
      "Persian Translation: تخصص .\n",
      "Persian Prediction: با    \n",
      "Evaluation translation (should we go to the mall?) : ('ار <\\\\s>',)\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m valid_indicies \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m persian_to_index[PADDING_TOKEN], \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m valid_indicies\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m---> 30\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#train_losses.append(loss.item())\u001b[39;00m\n",
      "File \u001b[1;32mu:\\UT\\6\\En2PerTranslation\\translation\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mu:\\UT\\6\\En2PerTranslation\\translation\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mu:\\UT\\6\\En2PerTranslation\\translation\\lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    769\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    770\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(device)\n",
    "total_loss = 0\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    iterator = iter(train_loader)\n",
    "    for batch_num, batch in enumerate(iterator):\n",
    "        transformer.train()\n",
    "        eng_batch, per_batch = batch\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(eng_batch, per_batch, num_heads)\n",
    "        optim.zero_grad()\n",
    "        persian_predictions = transformer(eng_batch,\n",
    "                                     per_batch,\n",
    "                                     encoder_self_attention_mask.to(device), \n",
    "                                     decoder_self_attention_mask.to(device), \n",
    "                                     decoder_cross_attention_mask.to(device),\n",
    "                                     encoder_start_token=False,\n",
    "                                     encoder_end_token=False,\n",
    "                                     decoder_start_token=True,\n",
    "                                     decoder_end_token=True)\n",
    "        labels = transformer.decoder.sentence_embedding.batch_tokenize(per_batch, start_token=False, end_token=True)\n",
    "        loss = criterion(\n",
    "            persian_predictions.view(-1, persian_vocab_size).to(device),\n",
    "            labels.view(-1).to(device)\n",
    "        ).to(device)\n",
    "        valid_indicies = torch.where(labels.view(-1) == persian_to_index[PADDING_TOKEN], False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        #train_losses.append(loss.item())\n",
    "        if batch_num % 100 == 0:\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"English: {eng_batch[0]}\")\n",
    "            print(f\"Persian Translation: {per_batch[0]}\")\n",
    "            persian_sentence_predicted = torch.argmax(persian_predictions[0], axis=1)\n",
    "            predicted_sentence = \"\"\n",
    "            for idx in persian_sentence_predicted:\n",
    "              if idx == persian_to_index[END_TOKEN]:\n",
    "                break\n",
    "              predicted_sentence += index_to_persian[idx.item()]\n",
    "            print(f\"Persian Prediction: {predicted_sentence}\")\n",
    "\n",
    "\n",
    "            transformer.eval()\n",
    "            persian_sentence = (\"\",)\n",
    "            eng_sentence = (\"should we go to the mall?\",)\n",
    "            for word_counter in range(max_sequence_length):\n",
    "                encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask= create_masks(eng_sentence, persian_sentence, num_heads)\n",
    "                predictions = transformer(eng_sentence,\n",
    "                                          persian_sentence,\n",
    "                                          encoder_self_attention_mask.to(device), \n",
    "                                          decoder_self_attention_mask.to(device), \n",
    "                                          decoder_cross_attention_mask.to(device),\n",
    "                                          encoder_start_token=False,\n",
    "                                          encoder_end_token=False,\n",
    "                                          decoder_start_token=True,\n",
    "                                          decoder_end_token=False)\n",
    "                next_token_prob_distribution = predictions[0][word_counter] # not actual probs\n",
    "                next_token_index = torch.argmax(next_token_prob_distribution).item()\n",
    "                next_token = index_to_persian[next_token_index]\n",
    "                persian_sentence = (persian_sentence[0] + next_token, )\n",
    "                if next_token == END_TOKEN:\n",
    "                  break\n",
    "            \n",
    "            print(f\"Evaluation translation (should we go to the mall?) : {persian_sentence}\")\n",
    "            print(\"-------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
